{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mahrokh/DECODE/Simulations/PAC-SNOC-dev\n",
      "[INFO] running on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahrokh/anaconda3/envs/NOC/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 Gaussian_biased_wide 8\n",
      "[INFO] bounding the loss to 1\n"
     ]
    }
   ],
   "source": [
    "import math, itertools, sys, os, pickle, copy\n",
    "from control import dlqr\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "print(BASE_DIR)\n",
    "sys.path.insert(1, BASE_DIR)\n",
    "\n",
    "from assistive_functions import *\n",
    "from experiments.scalar.loss_functions import LQLossFH\n",
    "from controllers.abstract import affine_controller\n",
    "from SVGD_src.approx_upperbound import approx_upper_bound\n",
    "from experiments.scalar.LTI_sys import LTI_system\n",
    "from experiments.scalar.scalar_assistive_functions import load_data, compute_posterior_by_gridding, heatmap_dists\n",
    "\n",
    "\n",
    "random_seed = 33\n",
    "random_state = np.random.RandomState(random_seed)\n",
    "logger = WrapLogger(None)\n",
    "\n",
    "# IMPORTANT CHOICES\n",
    "DEBUG = False\n",
    "epsilon = 0.2         # PAC holds with Pr >= 1-epsilon\n",
    "prior_type_b = 'Gaussian_biased_wide'\n",
    "S = 8\n",
    "n_grid = 65\n",
    "num_sampled_controllers = 20\n",
    "print(epsilon, prior_type_b, S)\n",
    "\n",
    "# ****** PART 1: GENERAL ******\n",
    "# ------ 1. load data ------\n",
    "T = 10\n",
    "dist_type = 'N biased'\n",
    "data_train, data_test, disturbance = load_data(\n",
    "    dist_type=dist_type, S=S, T=T, random_seed=random_seed,\n",
    "    S_test=None   # use a subset of available test data if not None\n",
    ")\n",
    "\n",
    "# ------ 2. define the plant ------\n",
    "sys = LTI_system(\n",
    "    A = np.array([[0.8]]),  # num_states*num_states\n",
    "    B = np.array([[0.1]]),  # num_states*num_inputs\n",
    "    C = np.array([[0.3]]),  # num_outputs*num_states\n",
    "    x_init = 2*np.ones((1, 1)),  # num_states*1\n",
    ")\n",
    "\n",
    "# ------ 3. define the loss ------\n",
    "Q = 5*torch.eye(sys.num_states).to(device)\n",
    "R = 0.003*torch.eye(sys.num_inputs).to(device)\n",
    "# optimal loss bound\n",
    "loss_bound = 1\n",
    "# sat_bound = np.matmul(np.matmul(np.transpose(sys.x_init), Q) , sys.x_init)\n",
    "sat_bound = torch.matmul(torch.matmul(torch.transpose(sys.x_init, 0, 1), Q), sys.x_init)\n",
    "if loss_bound is not None:\n",
    "    logger.info('[INFO] bounding the loss to ' + str(loss_bound))\n",
    "lq_loss_bounded = LQLossFH(Q, R, T, loss_bound, sat_bound, logger=logger)\n",
    "lq_loss_original = LQLossFH(Q, R, T, None, None, logger=logger)\n",
    "\n",
    "# ------ 4. Gibbs temperature ------\n",
    "gibbs_lambda_star = (8 * S * math.log(1/epsilon))**0.5        # lambda for Gibbs\n",
    "gibbs_lambda = gibbs_lambda_star\n",
    "\n",
    "# ****** PART 2: PRIOR ******\n",
    "\n",
    "# ------ prior on weight ------\n",
    "prior_type_w = 'Gaussian'\n",
    "prior_center = 'LQR-IH'\n",
    "gamma = 0\n",
    "delta = 1.0  # 2.5\n",
    "\n",
    "# get prior center\n",
    "if prior_center == 'LQR-IH':\n",
    "    K_lqr_ih, _, _ = dlqr(\n",
    "        sys.A.detach().numpy(), sys.B.detach().numpy(), \n",
    "        Q.detach().cpu().numpy(), R.detach().cpu().numpy()\n",
    "    )\n",
    "    theta_mid_grid = -K_lqr_ih\n",
    "    theta_mid_grid = torch.tensor(theta_mid_grid[0,0])\n",
    "# define different types of prior\n",
    "if prior_type_w == 'Gaussian':\n",
    "    prior_dict = {\n",
    "        'type_w':'Gaussian',\n",
    "        'weight_loc':theta_mid_grid*(1+gamma), 'weight_scale':delta,\n",
    "    }\n",
    "elif prior_type_w == 'Uniform':\n",
    "    prior_dict = {\n",
    "        'type_w':'Uniform',\n",
    "        'weight_low':theta_mid_grid*(1+gamma)-delta,\n",
    "        'weight_high':theta_mid_grid*(1+gamma)+delta,\n",
    "    }\n",
    "elif prior_type_w == 'Gaussian_trunc':\n",
    "    prior_dict = {\n",
    "        'type_w':'Gaussian_trunc',\n",
    "    }\n",
    "\n",
    "# ------ prior on bias ------\n",
    "if prior_type_b == 'Gaussian':\n",
    "    prior_dict.update({\n",
    "        'type_b':'Gaussian', \n",
    "        'bias_loc':0, 'bias_scale':2.5  # 5.0,\n",
    "    })\n",
    "elif prior_type_b == 'Uniform':\n",
    "    prior_dict.update({\n",
    "        'type_b':'Uniform', \n",
    "        'bias_low':-5, 'bias_high':5\n",
    "    })\n",
    "elif prior_type_b == 'Uniform_neg':\n",
    "    prior_dict.update({\n",
    "        'type_b':'Uniform_neg', \n",
    "        'bias_low':-5, 'bias_high':0\n",
    "    })\n",
    "elif prior_type_b == 'Uniform_pos':  # wrong prior\n",
    "    prior_dict.update({\n",
    "        'type_b':'Uniform_pos', \n",
    "        'bias_low':0, 'bias_high':5\n",
    "    })\n",
    "elif prior_type_b == 'Gaussian_biased':\n",
    "    prior_dict.update({\n",
    "        'type_b':'Gaussian_biased',\n",
    "        'bias_loc':-disturbance['mean'][0]/sys.B[0,0],\n",
    "        'bias_scale':1.0\n",
    "    })\n",
    "elif prior_type_b == 'Gaussian_biased_wide':\n",
    "    prior_dict.update({\n",
    "        'type_b':'Gaussian_biased',\n",
    "        'bias_loc':-disturbance['mean'][0]/sys.B[0,0],\n",
    "        'bias_scale':1.5\n",
    "    })\n",
    "elif prior_type_b == 'Gaussian_biased_narrow':\n",
    "    prior_dict.update({\n",
    "        'type_b':'Gaussian_biased',\n",
    "        'bias_loc':-disturbance['mean'][0]/sys.B[0,0],\n",
    "        'bias_scale':0.5\n",
    "    })\n",
    "\n",
    "for key, val in prior_dict.items():\n",
    "    if not isinstance(val, str):\n",
    "        prior_dict[key] = to_tensor(val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] calculating the posterior.\n"
     ]
    }
   ],
   "source": [
    "# ****** PART 3: POSTERIOR ******\n",
    "logger.info('[INFO] calculating the posterior.')\n",
    "res_dict = compute_posterior_by_gridding(\n",
    "    prior_dict=prior_dict, lq_loss_bounded=lq_loss_bounded,\n",
    "    data_train=data_train, dist_type=dist_type,\n",
    "    sys=sys, gibbs_lambda=gibbs_lambda, n_grid=n_grid\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    theta_grid = np.linspace(7, 3, n_grid)\n",
    "    bias_grid = np.linspace(7, 3, int((n_grid+1)/2))\n",
    "else:\n",
    "    theta_grid = np.array(res_dict['theta_grid'])\n",
    "    bias_grid = np.array(res_dict['bias_grid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Gaussian to Gibbs - Naive\n",
    "compute mean and var of the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Guassian fitting using moments:\n",
      "weight dist: N(-7.55, 0.77)\n",
      "bias dist: N(-2.81, 1.38)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "if DEBUG:\n",
    "    mu_theta, mu_bias = 5, 5\n",
    "    var_theta, var_bias = 0.5, 0.5\n",
    "else:\n",
    "    posterior_matrix = np.array(res_dict['posterior']).reshape(len(theta_grid), len(bias_grid))\n",
    "    # theta\n",
    "    posterior_proj_theta = np.sum(posterior_matrix, axis=1)\n",
    "    assert posterior_proj_theta.shape==theta_grid.flatten().shape\n",
    "    mu_theta = np.average(theta_grid.flatten(), weights=posterior_proj_theta)\n",
    "    var_theta = (np.average(theta_grid.flatten()**2, weights=posterior_proj_theta) - mu_theta**2)\n",
    "    # bias\n",
    "    posterior_proj_bias = np.sum(posterior_matrix, axis=0)\n",
    "    assert posterior_proj_bias.shape==bias_grid.shape\n",
    "    mu_bias = np.average(bias_grid.flatten(), weights=posterior_proj_bias)\n",
    "    var_bias = (np.average(bias_grid.flatten()**2, weights=posterior_proj_bias) - mu_bias**2)\n",
    "\n",
    "print('Naive Guassian fitting using moments:')\n",
    "print('weight dist: N({:.2f}, {:.2f})'.format(mu_theta, var_theta))\n",
    "print('bias dist: N({:.2f}, {:.2f})'.format(mu_bias, var_bias))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fr/b0ygwrg96p31crw71bnv66s40000gn/T/ipykernel_5896/631462915.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mean':torch.tensor(prior_dict['bias_loc']),\n",
      "/var/folders/fr/b0ygwrg96p31crw71bnv66s40000gn/T/ipykernel_5896/631462915.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mean':torch.tensor(prior_dict['weight_loc'].flatten()),\n"
     ]
    }
   ],
   "source": [
    "# vicont.generic_Gibbs.parameter_shapes()\n",
    "# OrderedDict([('out.bias', torch.Size([1])), ('out.weight', torch.Size([1]))])\n",
    "var_prior_dict ={\n",
    "    'out.bias':{\n",
    "        'mean':torch.tensor(prior_dict['bias_loc']),\n",
    "        'variance':torch.tensor(prior_dict['bias_scale']**2)\n",
    "    },\n",
    "    'out.weight':{\n",
    "        'mean':torch.tensor(prior_dict['weight_loc'].flatten()),\n",
    "        'variance':torch.tensor(prior_dict['weight_scale']**2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight dist: N(-7.56, 1.00)\n",
      "Initial bias dist: N(-3.00, 2.25)\n",
      "\n",
      "Iter 100/5000 - Time 37.04 sec - Loss 1.7160 - Av. Loss 1.7531\n",
      "updated best variational factors.\n",
      "\n",
      "Iter 200/5000 - Time 36.93 sec - Loss 1.6812 - Av. Loss 1.7634\n",
      "\n",
      "Iter 300/5000 - Time 36.84 sec - Loss 1.7227 - Av. Loss 1.7536\n",
      "\n",
      "Iter 400/5000 - Time 36.93 sec - Loss 1.7327 - Av. Loss 1.7274\n",
      "updated best variational factors.\n",
      "\n",
      "Iter 500/5000 - Time 36.76 sec - Loss 2.0005 - Av. Loss 1.7529\n",
      "\n",
      "Iter 600/5000 - Time 37.16 sec - Loss 1.6959 - Av. Loss 1.7482\n",
      "\n",
      "Iter 700/5000 - Time 36.89 sec - Loss 1.5289 - Av. Loss 1.7351\n",
      "\n",
      "Iter 800/5000 - Time 36.90 sec - Loss 1.8483 - Av. Loss 1.7675\n",
      "\n",
      "Iter 900/5000 - Time 36.80 sec - Loss 1.6976 - Av. Loss 1.7418\n",
      "\n",
      "Iter 1000/5000 - Time 36.79 sec - Loss 1.5607 - Av. Loss 1.7381\n",
      "\n",
      "Iter 1100/5000 - Time 41.44 sec - Loss 1.9418 - Av. Loss 1.7656\n",
      "\n",
      "Iter 1200/5000 - Time 37.11 sec - Loss 2.0077 - Av. Loss 1.7488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial weight dist: N(\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     23\u001b[0m     var_prior\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     24\u001b[0m     math\u001b[38;5;241m.\u001b[39mexp(var_prior\u001b[38;5;241m.\u001b[39mscale_raw\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     25\u001b[0m ))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitial bias dist: N(\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     27\u001b[0m     var_prior\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m0\u001b[39m], math\u001b[38;5;241m.\u001b[39mexp(var_prior\u001b[38;5;241m.\u001b[39mscale_raw\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m ))\n\u001b[0;32m---> 30\u001b[0m vicont\u001b[38;5;241m.\u001b[39mfit(log_period\u001b[38;5;241m=\u001b[39mlog_period, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained weight dist: N(\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     33\u001b[0m     vicont\u001b[38;5;241m.\u001b[39mvar_post\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m1\u001b[39m], math\u001b[38;5;241m.\u001b[39mexp(vicont\u001b[38;5;241m.\u001b[39mvar_post\u001b[38;5;241m.\u001b[39mscale_raw\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     34\u001b[0m ))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained bias dist: N(\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     36\u001b[0m     vicont\u001b[38;5;241m.\u001b[39mvar_post\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m0\u001b[39m], math\u001b[38;5;241m.\u001b[39mexp(vicont\u001b[38;5;241m.\u001b[39mvar_post\u001b[38;5;241m.\u001b[39mscale_raw\u001b[38;5;241m.\u001b[39mdetach()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     37\u001b[0m ))\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/controllers/VI_controller.py:101\u001b[0m, in \u001b[0;36mVICont.fit\u001b[0;34m(self, early_stopping, log_period)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    100\u001b[0m \u001b[39m# try:\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_neg_elbo(task_dict_batch)\n\u001b[1;32m    102\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/controllers/VI_controller.py:204\u001b[0m, in \u001b[0;36mVICont.get_neg_elbo\u001b[0;34m(self, tasks_dicts)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# tile data to number or VFs\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39m# data_tuples_tiled = _tile_data_tuples(tasks_dicts, self.num_vfs)\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     data_tuples_tiled \u001b[39m=\u001b[39m tasks_dicts \u001b[39m#TODO: use the above line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     log_posterior_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneric_Gibbs\u001b[39m.\u001b[39mlog_prob(param_sample, data_tuples_tiled)    \u001b[39m# log\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     log_var_post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar_post\u001b[39m.\u001b[39mlog_prob(param_sample)\n\u001b[1;32m    206\u001b[0m     elbo \u001b[39m=\u001b[39m log_posterior_num \u001b[39m-\u001b[39m log_var_post\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/SVGD_src/distributions.py:176\u001b[0m, in \u001b[0;36mGibbsPosterior.log_prob\u001b[0;34m(self, params, train_data)\u001b[0m\n\u001b[1;32m    173\u001b[0m     params \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    174\u001b[0m L \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 176\u001b[0m lpl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_prob_likelihood(params, train_data)\n\u001b[1;32m    177\u001b[0m lpl \u001b[39m=\u001b[39m lpl\u001b[39m.\u001b[39mreshape(L)\n\u001b[1;32m    178\u001b[0m lpp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_prob_prior(params)\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/SVGD_src/distributions.py:156\u001b[0m, in \u001b[0;36mGibbsPosterior._log_prob_likelihood\u001b[0;34m(self, params, train_data)\u001b[0m\n\u001b[1;32m    152\u001b[0m cl_system\u001b[39m.\u001b[39mcontroller\u001b[39m.\u001b[39mset_parameters_as_vector(\n\u001b[1;32m    153\u001b[0m     params[l_tmp, :]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[39m# rollout\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m xs, _, us \u001b[39m=\u001b[39m cl_system\u001b[39m.\u001b[39mmulti_rollout(train_data)\n\u001b[1;32m    157\u001b[0m \u001b[39m# compute loss\u001b[39;00m\n\u001b[1;32m    158\u001b[0m loss_val_tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn\u001b[39m.\u001b[39mforward(xs, us)\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/controllers/abstract.py:32\u001b[0m, in \u001b[0;36mCLSystem.multi_rollout\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m     x_tmp, y_tmp, u_tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msys\u001b[39m.\u001b[39mrollout(\n\u001b[1;32m     28\u001b[0m         controller\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller,\n\u001b[1;32m     29\u001b[0m         data\u001b[39m=\u001b[39mdata[sample_num, :, :], train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     x_tmp, y_tmp, u_tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msys\u001b[39m.\u001b[39mrollout(\n\u001b[1;32m     33\u001b[0m         controller\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller,\n\u001b[1;32m     34\u001b[0m         data\u001b[39m=\u001b[39mdata[sample_num, :, :]\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m sample_num\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     37\u001b[0m     xs \u001b[39m=\u001b[39m x_tmp\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mx_tmp\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/experiments/scalar/LTI_sys.py:66\u001b[0m, in \u001b[0;36mLTI_system.rollout\u001b[0;34m(self, controller, data, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     xs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     58\u001b[0m         (xs, (torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA, xs[t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB, us[t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]) \u001b[39m+\u001b[39m data[t, :])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()),\n\u001b[1;32m     59\u001b[0m         \u001b[39m0\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     ys \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     62\u001b[0m         (ys, torch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC, xs[t, :])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()),\n\u001b[1;32m     63\u001b[0m         \u001b[39m0\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m     us \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m---> 66\u001b[0m         (us, controller\u001b[39m.\u001b[39mforward(xs[t, :])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()),\n\u001b[1;32m     67\u001b[0m         \u001b[39m0\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m xs, ys, us\n",
      "File \u001b[0;32m~/DECODE/Simulations/PAC-SNOC-dev/controllers/vectorized_controller.py:190\u001b[0m, in \u001b[0;36mControllerVectorized.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m         prev_size \u001b[39m=\u001b[39m size\n\u001b[1;32m    183\u001b[0m     \u001b[39msetattr\u001b[39m(\n\u001b[1;32m    184\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    185\u001b[0m         LinearVectorized(\n\u001b[1;32m    186\u001b[0m             prev_size, num_inputs,\n\u001b[1;32m    187\u001b[0m             requires_bias\u001b[39m=\u001b[39mrequires_bias[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    188\u001b[0m             nonlinearity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinearity_output))\n\u001b[0;32m--> 190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    191\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m    The `forward` function takes an input `x`, passes it through a series of fully connected layers\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    and non-linear activation functions, and returns the final output.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     output \u001b[39m=\u001b[39m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from controllers.VI_controller import VICont\n",
    "\n",
    "lr = 1e-2\n",
    "num_iter_fit = 20000 if DEBUG else 5000  #TODO: increase\n",
    "log_period = 1000 if DEBUG else 100\n",
    "# initialize variational inference with the prior\n",
    "\n",
    "vicont = VICont(\n",
    "    sys, train_d=data_train, lr=lr, loss=lq_loss_bounded, \n",
    "    prior_dict=prior_dict, controller_type='NN',\n",
    "    random_seed=random_seed, optimizer='Adam', batch_size=-1, lambda_=gibbs_lambda,\n",
    "    num_iter_fit=num_iter_fit, lr_decay=1.0, logger=None,\n",
    "    # VI properties\n",
    "    num_vfs=1, vf_init_std=0.1, vf_cov_type='diag', L=10,\n",
    "    vf_param_dists=var_prior_dict,  # intialize with the prior\n",
    "    # NN controller properties\n",
    "    layer_sizes=[], nonlinearity_hidden=None, nonlinearity_output=None,\n",
    "    # debug\n",
    "    debug=DEBUG\n",
    ")\n",
    "var_prior = copy.deepcopy(vicont.var_post)\n",
    "print('Initial weight dist: N({:.2f}, {:.2f})'.format(\n",
    "    var_prior.loc.detach()[1], \n",
    "    math.exp(var_prior.scale_raw.detach()[1])**2\n",
    "))\n",
    "print('Initial bias dist: N({:.2f}, {:.2f})'.format(\n",
    "    var_prior.loc.detach()[0], math.exp(var_prior.scale_raw.detach()[0])**2\n",
    "))\n",
    "\n",
    "vicont.fit(log_period=log_period, early_stopping=True)\n",
    "\n",
    "print('Trained weight dist: N({:.2f}, {:.2f})'.format(\n",
    "    vicont.var_post.loc.detach()[1], math.exp(vicont.var_post.scale_raw.detach()[1])**2\n",
    "))\n",
    "print('Trained bias dist: N({:.2f}, {:.2f})'.format(\n",
    "    vicont.var_post.loc.detach()[0], math.exp(vicont.var_post.scale_raw.detach()[0])**2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_surface = np.mean(-theta_grid[1:]+theta_grid[:-1])*np.mean(-bias_grid[1:]+bias_grid[:-1])\n",
    "dist_names = ['var_post', 'var_prior', 'moments matching']\n",
    "for dist_name in dist_names:\n",
    "    if dist_name=='moments matching':\n",
    "        mean_w, mean_b = mu_theta, mu_bias\n",
    "        sigma_w, sigma_b = var_theta**0.5, var_bias**0.5\n",
    "    else:\n",
    "        var_dist = var_prior if dist_name=='var_prior' else vicont.var_post\n",
    "        if dist_name=='var_prior' and DEBUG:\n",
    "            res_dict['var_prior']=None\n",
    "            continue\n",
    "        mean_w = var_dist.loc.detach().numpy()[1]\n",
    "        sigma_w = np.exp(var_dist.scale_raw.detach().numpy()[1])\n",
    "        mean_b = var_dist.loc.detach().numpy()[0]\n",
    "        sigma_b = np.exp(var_dist.scale_raw.detach().numpy()[0])\n",
    "       \n",
    "    vi_w = 1/(sigma_w*np.sqrt(2*np.pi)) * np.exp(-(theta_grid-mean_w)**2/(2*sigma_w**2))\n",
    "    # vi_w = vi_w * abs(theta_grid[-1]-theta_grid[0])/len(theta_grid)\n",
    "    vi_w = vi_w.flatten()\n",
    "\n",
    "    vi_b = 1/(sigma_b*np.sqrt(2*np.pi)) * np.exp(-(bias_grid-mean_b)**2/(2*sigma_b**2))\n",
    "    vi_b = vi_b * abs(bias_grid[-1]-bias_grid[0])/len(bias_grid)\n",
    "    vi_b = vi_b.flatten()\n",
    "    res_dict[dist_name] = [a[0]*a[1] for a in itertools.product(vi_w, vi_b)]\n",
    "    res_dict[dist_name] = res_dict[dist_name]/(sum(res_dict[dist_name])+1e-6)\n",
    "    assert len(res_dict[dist_name])==len(res_dict['prior'])\n",
    "    assert abs(sum(res_dict[dist_name])-1)<=1e-4, sum(res_dict[dist_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [\n",
    "    res_dict['var_prior'], res_dict['var_post'], \n",
    "    res_dict['moments matching'],res_dict['posterior'],\n",
    "]\n",
    "\n",
    "titles=[\n",
    "    'Variational Prior', 'Variational Posterior', \n",
    "    'Gaussian with same first two moments', 'Gridded Posterior'\n",
    "]\n",
    "if DEBUG:\n",
    "    dists, titles = dists[1:], titles[1:]\n",
    "    \n",
    "heatmap_dists(\n",
    "    dists=dists, theta_grid=theta_grid, bias_grid=bias_grid, \n",
    "    extend_neg=not DEBUG, titles=titles, \n",
    "    save_fig=False, marker_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximated_Z_var = np.array(res_dict['posterior_unnorm'])/np.array(res_dict['var_post'])\n",
    "\n",
    "print(np.mean(approximated_Z_var))\n",
    "print(res_dict['approximated_Z'])\n",
    "print(np.min(approximated_Z_var), np.max(approximated_Z_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = [\n",
    "    [res_dict['var_post'][ind]/res_dict['posterior'][ind] for ind in range(len(res_dict['posterior']))],\n",
    "    approximated_Z_var-res_dict['approximated_Z']\n",
    "]\n",
    "titles=[\n",
    "    'Variational Posterior / Gridded Posterior', 'Z_var - Z_grid'\n",
    "]\n",
    "    \n",
    "heatmap_dists(\n",
    "    dists=dists, theta_grid=theta_grid, bias_grid=bias_grid, \n",
    "    extend_neg=True, titles=titles, \n",
    "    save_fig=False, marker_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "sample_from = 'uniform'\n",
    "\n",
    "if sample_from=='uniform':\n",
    "    sampled_inds = random_state.choice(\n",
    "        len(res_dict['posterior']), num_samples, replace=True\n",
    "    )\n",
    "elif sample_from=='var_post':\n",
    "    sampled_inds = random_state.choice(\n",
    "        len(res_dict['posterior']), num_samples, replace=True,\n",
    "        p = res_dict['var_post']/sum(res_dict['var_post'])\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError   \n",
    "\n",
    "approx_Z_theta = np.array([\n",
    "    res_dict['posterior_unnorm'][i]/res_dict['var_post'][i] for i in sampled_inds\n",
    "])\n",
    "\n",
    "# print(approx_Z_theta)\n",
    "msg = 'Approximating the partition function (Z) using {:.0f} samples '.format(\n",
    "    num_samples\n",
    ")\n",
    "msg += 'using '+sample_from+' sampling method.'\n",
    "print(msg)\n",
    "\n",
    "# 1. uniform weighting\n",
    "print('Weighting uniformly: ', np.mean(approx_Z_theta))\n",
    "\n",
    "# 2. weighting by unnormalized posterior\n",
    "weights = np.array([\n",
    "    res_dict['posterior_unnorm'][i] for i in sampled_inds\n",
    "])\n",
    "weights = weights/sum(weights)\n",
    "print(\n",
    "    'Weighting according to unnormalized posterior: ', \n",
    "    np.sum(approx_Z_theta*weights)\n",
    ")\n",
    "\n",
    "# 3. weighting by variational posterior\n",
    "weights = np.array([\n",
    "    res_dict['var_post'][i] for i in sampled_inds\n",
    "])\n",
    "weights = weights/sum(weights)\n",
    "print(\n",
    "    'Weighting according to variational posterior: ', \n",
    "    np.sum(approx_Z_theta*weights)\n",
    ")\n",
    "\n",
    "print('True value = ', res_dict['approximated_Z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: similarly, can sample from VI and weight uniformly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('NOC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cea5c68dc0355c09225c58fbb8834b79a134565fb5da138a8afd8fbd1a5df1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
