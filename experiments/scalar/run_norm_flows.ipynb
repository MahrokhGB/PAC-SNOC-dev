{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows for the scalar example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] running on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahrokh/anaconda3/envs/NOC/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import torch, math\n",
    "import numpy as np\n",
    "import normflows as nf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys, os, pickle\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(BASE_DIR)\n",
    "from assistive_functions import *\n",
    "from experiments.scalar.loss_functions import LQLossFH\n",
    "from controllers.abstract import get_controller\n",
    "from experiments.scalar.LTI_sys import LTI_system\n",
    "from inference_algs.distributions import GibbsPosterior\n",
    "from experiments.scalar.scalar_assistive_functions import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 Gaussian_biased_wide 8\n",
      "[INFO] bounding the loss to 1\n"
     ]
    }
   ],
   "source": [
    "# ****** PART 1: GENERAL ******\n",
    "random_seed = 33\n",
    "random_state = np.random.RandomState(random_seed)\n",
    "logger = WrapLogger(None)\n",
    "\n",
    "# ------ 1. load data ------\n",
    "T = 10\n",
    "S = 8\n",
    "epsilon = 0.2       # PAC holds with Pr >= 1-epsilon\n",
    "dist_type = 'N biased'\n",
    "prior_type_b = 'Gaussian_biased_wide'\n",
    "\n",
    "data_train, data_test, disturbance = load_data(\n",
    "    dist_type=dist_type, S=S, T=T, random_seed=random_seed,\n",
    "    S_test=None     # use a subset of available test data if not None\n",
    ")\n",
    "\n",
    "print(epsilon, prior_type_b, S)\n",
    "\n",
    "# ------ 2. define the plant ------\n",
    "sys = LTI_system(\n",
    "    A = np.array([[0.8]]),  # num_states*num_states\n",
    "    B = np.array([[0.1]]),  # num_states*num_inputs\n",
    "    C = np.array([[0.3]]),  # num_outputs*num_states\n",
    "    x_init = 2*np.ones((1, 1)),  # num_states*1\n",
    ")\n",
    "\n",
    "# ------ 3. define the loss ------\n",
    "Q = 5*torch.eye(sys.num_states).to(device)\n",
    "R = 0.003*torch.eye(sys.num_inputs).to(device)\n",
    "# optimal loss bound\n",
    "loss_bound = 1\n",
    "# sat_bound = np.matmul(np.matmul(np.transpose(sys.x_init), Q) , sys.x_init)\n",
    "sat_bound = torch.matmul(torch.matmul(torch.transpose(sys.x_init, 0, 1), Q), sys.x_init)\n",
    "if loss_bound is not None:\n",
    "    logger.info('[INFO] bounding the loss to ' + str(loss_bound))\n",
    "lq_loss_bounded = LQLossFH(Q, R, T, loss_bound, sat_bound, logger=logger)\n",
    "lq_loss_original = LQLossFH(Q, R, T, None, None, logger=logger)\n",
    "\n",
    "# ------ 4. Gibbs temperature ------\n",
    "gibbs_lambda_star = (8 * S * math.log(1/epsilon))**0.5        # lambda for Gibbs\n",
    "\n",
    "# ------ 5. controller ------\n",
    "# define a generic controller\n",
    "generic_controller = get_controller(\n",
    "    controller_type='Affine', sys=sys,\n",
    "    # initialization_std=0.1, # for initializing REN. not important\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target distribution\n",
    "gibbs_posteior = GibbsPosterior(\n",
    "    loss_fn=lq_loss_bounded, lambda_=gibbs_lambda_star, prior_dict, initialization_std,\n",
    "    # attributes of the CL system\n",
    "    controller_type, sys,\n",
    "    # REN controller\n",
    "    n_xi=None, l=None, x_init=None, u_init=None,\n",
    "    # misc\n",
    "    logger=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 16\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "# Move model on GPU if available\n",
    "enable_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')\n",
    "\n",
    "flows = []\n",
    "for i in range(K):\n",
    "    flows += [nf.flows.Planar((2,))]\n",
    "target = nf.distributions.TwoModes(2, 0.1)\n",
    "\n",
    "q0 = nf.distributions.DiagGaussian(2)\n",
    "nfm = nf.NormalizingFlow(q0=q0, flows=flows, p=target)\n",
    "nfm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load\n",
    "file_path = os.path.join(BASE_DIR, 'experiments', 'scalar', 'saved_results')\n",
    "filename = dist_type.replace(\" \", \"_\")+'_ours_'+prior_type+'_T'+str(T)+'_S'+str(S)+'_eps'+str(int(eps_heat*10))+'.pkl'\n",
    "filename = os.path.join(file_path, filename)\n",
    "filehandler = open(filename, 'rb')\n",
    "res_dict = pickle.load(filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "res_dict['theta_grid'] = [k[0,0] for k in res_dict['theta_grid']]\n",
    "res_dict['theta'] = [k[0,0] for k in res_dict['theta']]\n",
    "\n",
    "theta_grid = - np.array(res_dict['theta_grid'])\n",
    "bias_grid = - np.array(res_dict['bias_grid'])\n",
    "Z_posterior = np.reshape(\n",
    "    np.array(res_dict['posterior']),\n",
    "    (len(theta_grid), len(bias_grid))\n",
    ")\n",
    "assert abs(sum(sum(Z_posterior))-1)<=1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "# grid_size = 200\n",
    "# xx, yy = torch.meshgrid(X, torch.linspace(-3, 3, grid_size))\n",
    "# z = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\n",
    "# log_prob = target.log_prob(z.to(device)).to('cpu').view(*xx.shape)\n",
    "# prob = torch.exp(log_prob)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pcolormesh(bias_grid, theta_grid, Z_posterior, shading='nearest')\n",
    "plt.show()\n",
    "\n",
    "# Plot initial flow distribution\n",
    "z, _ = nfm.sample(num_samples=2 ** 20)\n",
    "z_np = z.to('cpu').data.numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.hist2d(\n",
    "    z_np[:, 0].flatten(), z_np[:, 1].flatten(), \n",
    "    (len(bias_grid), len(theta_grid)), \n",
    "    range=[[bias_grid[0], bias_grid[-1]], [theta_grid[0], theta_grid[-1]]]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "max_iter = 20000\n",
    "num_samples = 2 * 20\n",
    "anneal_iter = 10000\n",
    "annealing = True\n",
    "show_iter = 2000\n",
    "\n",
    "\n",
    "loss_hist = np.array([])\n",
    "\n",
    "optimizer = torch.optim.Adam(nfm.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "for it in tqdm(range(max_iter)):\n",
    "    optimizer.zero_grad()\n",
    "    if annealing:\n",
    "        loss = nfm.reverse_kld(num_samples, beta=np.min([1., 0.01 + it / anneal_iter]))\n",
    "    else:\n",
    "        loss = nfm.reverse_kld(num_samples)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
    "    \n",
    "    # Plot learned distribution\n",
    "    if (it + 1) % show_iter == 0:\n",
    "        torch.cuda.manual_seed(0)\n",
    "        z, _ = nfm.sample(num_samples=2 ** 20)\n",
    "        z_np = z.to('cpu').data.numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.hist2d(z_np[:, 0].flatten(), z_np[:, 1].flatten(), (grid_size, grid_size), range=[[-3, 3], [-3, 3]])\n",
    "        plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(loss_hist, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot learned distribution\n",
    "z, _ = nfm.sample(num_samples=2 ** 20)\n",
    "z_np = z.to('cpu').data.numpy()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.hist2d(z_np[:, 0].flatten(), z_np[:, 1].flatten(), (grid_size, grid_size), range=[[-3, 3], [-3, 3]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 ('NOC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "3cea5c68dc0355c09225c58fbb8834b79a134565fb5da138a8afd8fbd1a5df1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
